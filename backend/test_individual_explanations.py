#!/usr/bin/env python3
"""
Test script for individual metric explanations functionality
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.services import analysis_service, llm_service

def test_individual_explanations():
    """Test the individual metric explanations with sample data"""
    
    # Sample metrics data (similar to what OCR service would extract)
    sample_metrics = [
        {
            "name": "–ì–µ–º–æ–≥–ª–æ–±–∏–Ω",
            "value": 140.0,
            "unit": "–≥/–ª",
            "reference_range": "120-160",
            "status": "normal"
        },
        {
            "name": "–ì–ª—é–∫–æ–∑–∞",
            "value": 110.0,
            "unit": "–º–≥/–¥–ª",
            "reference_range": "70-99",
            "status": "elevated"
        },
        {
            "name": "–•–æ–ª–µ—Å—Ç–µ—Ä–∏–Ω",
            "value": 4.2,
            "unit": "–º–º–æ–ª—å/–ª",
            "reference_range": "3.0-5.0",
            "status": "normal"
        }
    ]
    
    print("üß™ Testing Individual Metric Explanations")
    print("=" * 50)
    
    # Test 1: Individual metric explanations
    print("\n1. Testing individual metric explanations...")
    try:
        explained_metrics = llm_service.generate_individual_metric_explanations(sample_metrics)
        
        for metric in explained_metrics:
            print(f"\nüìä {metric['name']}: {metric['value']} {metric['unit']}")
            print(f"   Status: {metric['status']}")
            print(f"   Explanation: {metric.get('explanation', 'No explanation generated')}")
            
        print(f"\n‚úÖ Successfully generated explanations for {len(explained_metrics)} metrics")
        
    except Exception as e:
        print(f"‚ùå Error in individual explanations: {e}")
        return False
    
    # Test 2: Analysis from text
    print("\n2. Testing analysis from text...")
    sample_text = """
    –ì–µ–º–æ–≥–ª–æ–±–∏–Ω: 140 –≥/–ª (–Ω–æ—Ä–º–∞: 120-160)
    –ì–ª—é–∫–æ–∑–∞: 110 –º–≥/–¥–ª (–Ω–æ—Ä–º–∞: 70-99)
    –•–æ–ª–µ—Å—Ç–µ—Ä–∏–Ω: 4.2 –º–º–æ–ª—å/–ª (–Ω–æ—Ä–º–∞: 3.0-5.0)
    """
    
    try:
        result = analysis_service.analyze_metrics_from_text_with_explanations(sample_text)
        
        print(f"üìù Overall Summary: {result['overall_summary']}")
        print(f"üìà Metrics found: {len(result['metrics'])}")
        
        for metric in result['metrics']:
            print(f"   ‚Ä¢ {metric['name']}: {metric['status']}")
            
        print("‚úÖ Successfully analyzed text with explanations")
        
    except Exception as e:
        print(f"‚ùå Error in text analysis: {e}")
        return False
    
    # Test 3: Status grouping
    print("\n3. Testing metric grouping by status...")
    try:
        grouped = analysis_service.get_metrics_by_status(explained_metrics)
        
        for status, metrics in grouped.items():
            if metrics:
                print(f"   {status.upper()}: {len(metrics)} metrics")
                for metric in metrics:
                    print(f"     - {metric['name']}")
                    
        print("‚úÖ Successfully grouped metrics by status")
        
    except Exception as e:
        print(f"‚ùå Error in metric grouping: {e}")
        return False
    
    print("\nüéâ All tests completed successfully!")
    return True

def test_fallback_explanations():
    """Test fallback explanations when API is not available"""
    print("\nüîß Testing fallback explanations...")
    
    sample_metric = {
        "name": "Test Metric",
        "value": 100.0,
        "unit": "mg/dl",
        "reference_range": "80-120",
        "status": "normal"
    }
    
    try:
        fallback = llm_service._get_fallback_metric_explanation(sample_metric)
        print(f"üìù Fallback explanation: {fallback}")
        print("‚úÖ Fallback explanations working correctly")
        return True
    except Exception as e:
        print(f"‚ùå Error in fallback explanations: {e}")
        return False

if __name__ == "__main__":
    print("üöÄ Starting Individual Metric Explanations Tests")
    
    # Test fallback functionality first (doesn't require API)
    fallback_success = test_fallback_explanations()
    
    # Test main functionality
    main_success = test_individual_explanations()
    
    if fallback_success and main_success:
        print("\n‚úÖ All tests passed! Individual metric explanations are working correctly.")
        sys.exit(0)
    else:
        print("\n‚ùå Some tests failed. Check the output above for details.")
        sys.exit(1) 